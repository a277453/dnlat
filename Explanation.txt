WHAT EACH MODULE DOES
routes.py - Traffic controller - receives requests, calls services, returns responses
extraction.py - Unzips files, filters relevant ones, creates temp directory
categorization.py - Sorts files into buckets (customer_journals, ui_journals, etc.)
configManager.py - Parses XML config, detects file types using pattern matching
processing.py - Formats categorization results into response structure
session.py - In-memory database - stores/retrieves temporary data
transaction_analyzer.py - Parses customer journals, extracts transactions, calculates stats
ui_journal_processor.py - Parses UI journals, extracts screen flows, removes duplicates
registry_analyzer.py - Parses .reg files, compares two registry files
flow_visualization.py - Creates Plotly charts for transaction flows
schemas.py - Defines data structures (request/response formats)
app.py - Streamlit UI - displays forms, tables, charts, handles user input



FastAPI backend (12 REST endpoints)
Streamlit frontend (8 analysis functions)
Modular architecture (9 service modules)
LCS algorithm for intelligent flow matching



Main API Endpoints:

POST /process-zip
Receives ZIP file upload
Extracts → Categorizes → Stores in session
Returns file counts by category

GET /available-file-types
Returns what file types were found in the ZIP
Uses session data

POST /select-file-type
User selects which file type to work with
Stores selection in session

POST /analyze-customer-journals
Parses transaction logs from customer journal files
Extracts: Transaction ID, Type, Start/End times, State (Success/Fail)
Stores parsed data in session

GET /get-transactions-with-sources
Returns all transactions with their source file names
Used for filtering

POST /filter-transactions-by-sources
Filters transactions by selected source files

GET /transaction-statistics
Generates stats: success rates, counts by type

POST /compare-transactions-flow
Compares UI flows of two transactions
Uses Longest Common Subsequence (LCS) algorithm to find matching screens
Returns side-by-side comparison

POST /visualize-individual-transaction-flow
Gets UI flow for a single transaction
Calculates durations between screens

POST /generate-consolidated-flow
Aggregates flows from multiple transactions of same type
Shows common paths through UI screens

POST /analyze-transaction-llm
Sends transaction log to Ollama LLM for analysis
Uses model: "llama3_log_analyzer"
Returns AI-generated anomaly detection

POST /submit-llm-feedback
Stores user feedback on LLM analysis
Saves to llm_feedback.json file

GET /get-feedback/{transaction_id}
Retrieves all feedback for a transaction

POST /get-counter-data
Extracts counter data from TRC trace files
Maps counters to transaction timestamps
Shows cash cassette states (before/after transaction)

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 1: ZIP FILE UPLOAD & PROCESSING

User uploads ZIP file in Streamlit

File stays in browser memory temporarily
Streamlit sends it to backend via HTTP POST

Backend receives the ZIP

FastAPI endpoint /process-zip catches the upload
Validates it's actually a ZIP file

Extraction happens

Creates unique temp folder: /tmp/dn_extracts/dn_1735824000_abc123xyz/
Opens ZIP in memory (doesn't write to disk first)
Checks every file inside ZIP against "relevant patterns"
Only extracts files matching: customerjournal, uijournal, .trc, .reg, .jrn, .prn
Skips Mac junk files: __MACOSX, .DS_Store

File type detection

For each extracted file, reads first 100-200 lines
Uses pattern matching to identify file type:

Customer Journal: Simple format, no special characters, specific TIDs
UI Journal: Has JSON, direction symbols (<>*), square brackets
TRC Trace: Millisecond timestamps, PID patterns
Registry: .reg extension or .txt with "reg" in name

Counts how many lines match each pattern
Whichever pattern has most matches wins

Categorization

Creates 5 empty buckets: customer_journals, ui_journals, trc_trace, trc_error, registry_files
Puts each file in correct bucket based on detection result

Organization

Creates subdirectories: /tmp/.../customer_journals/, /tmp/.../ui_journals/, etc.
Copies files from original location into category folders
Now files are organized by type

Store in session

Saves everything in memory dictionary:
Which files exist
Where they're located
Empty placeholders for future analysis


Uses session ID "current_session"

Return to frontend

Sends JSON response with file counts
Frontend shows: "Customer Journals: 1, UI Journals: 1, etc."

Where files physically exist:

Original ZIP: User's computer
Extracted files: /tmp/dn_extracts/dn_1735824000_abc123xyz/
Organized files: /tmp/dn_extracts/.../customer_journals/, etc.
Session data: Backend server RAM

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 2: TRANSACTION ANALYSIS

User clicks "Transaction Type Statistics"

Frontend asks backend: "Do you have transaction data?"
Backend checks session, says: "No, not yet"

Auto-analyze triggered

Frontend automatically calls /analyze-customer-journals
Shows loading spinner

Backend starts parsing

Gets list of customer journal files from session
Loads XML config file to learn transaction codes

XML config tells parser:

"COUT" means "Cash Withdrawal"
"BAL" means "Balance Inquiry"
TID 3201/3207 = transaction start
TID 3202 = transaction end
TID 3239 = chained transaction

For each journal file:

Opens file, reads all lines
Parses each line: extracts timestamp, TID, message

Finding transactions:

Scans through lines looking for start TIDs (3201 or 3207)
When found, marks position
Keeps scanning until finds end TID (3202)
Now knows transaction boundaries (line 0 to line 3, for example)

For each transaction found:

Extract ID: Looks for "Transaction no. '001'" in start line
Extract type: Looks for "Function 'COUT'" in lines, converts to "Cash Withdrawal"
Extract times: Gets timestamp from start line and end line
Determine success: Looks for "end-state'N'" (success) or "end-state'E'" (fail)
Calculate duration: End time minus start time
Collect log: Joins all lines between start and end

Creates list of transaction dictionaries
Saves to session under 'transaction_data'
Generates statistics: count by type, success rates

Return to frontend

Sends statistics as JSON
Frontend displays in tables and charts

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 3: TRANSACTION COMPARISON

User selects two transactions to compare

Picks from dropdown: TXN001 and TXN002
Frontend sends both IDs to backend

Backend retrieves transaction data

Gets both transaction records from session
Each has: ID, type, start/end times, state, log

Find matching UI journals

Each transaction has a source file (e.g., "20250404")
Looks for UI journal with same date in filename
Opens the matching UI journal file

Parse UI journal

Reads file line by line
Extracts: timestamp, screen name, event type, JSON data
Example line: "12:00:01 GUIAPP > [123] - Login result:{...}"
Becomes: {time: 12:00:01, screen: "Login"}

Get screen flow for each transaction

Filters UI events between transaction start and end times
Removes duplicate consecutive screens
TXN001: ['Login', 'PIN', 'Menu', 'Amount', 'Cash', 'Receipt']
TXN002: ['Login', 'PIN', 'Menu', 'Amount', 'Error', 'Cancel']

Calculate durations
For each screen, finds first occurrence time
Calculates time until next screen appears
Example: Login (12:00:01) → PIN (12:00:05) = 4 seconds

Find matches using LCS algorithm

Compares two screen lists
Finds screens that appear in SAME ORDER in both
Not just "exists in both" - must be sequential
Common: Login, PIN, Menu, Amount (in order)
Different: Cash+Receipt vs Error+Cancel

Mark matches in both flows

Creates boolean arrays: [True, True, True, True, False, False]
True = screen matches, False = unique to this transaction

Return comparison

Sends both flows with match flags
Frontend displays side-by-side with color coding:

Green checkmark = matches
Orange warning = unique

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 4: INDIVIDUAL UI FLOW VISUALIZATION

User selects a single transaction

Picks from dropdown
Frontend requests flow data

Backend gets transaction details

Retrieves from session: start time, end time, source file

Find matching UI journal

Looks for UI journal with same date as transaction

Extract screen flow

Filters UI events in transaction time range
Gets unique screens in order
Calculates duration for each screen

Calculate durations

Create flowchart data

Return to frontend

Frontend uses Plotly to draw

Result: User sees visual flowchart showing customer journey through UI screens

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 5: CONSOLIDATED FLOW

User selects source file + transaction type

Example: "20250404" + "Cash Withdrawal"

Backend filters transactions

Gets all transactions matching source + type
Example: 30 withdrawal transactions

Extract flow for EACH transaction

For each of 30 transactions:

Get start/end time
Filter UI events
Extract screen list

Aggregate all flows

Collects all unique screens across all transactions
Tracks transitions: how many times did "Menu" → "Amount"
Records which transactions visited each screen

Build consolidated view

Screens: All unique screens found
Transitions: Count of each screen-to-screen movement
Transaction map: Which transactions hit each screen

Create diagram

Frontend draws network diagram:

Boxes = screens
Arrows = transitions (with counts)
Hover shows which transactions

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 6: LLM ANALYSIS

User selects transaction + clicks "Analyze"

Frontend sends transaction ID

Backend gets transaction log

Retrieves full log text from session
Example: 500 lines of timestamped events

Call Ollama LLM

Sends to local Ollama server (running on port 11434)
Uses model: "llama3_log_analyzer"
System prompt: "You are a log analysis expert..."
User prompt: "Analyze this log: [500 lines]"

LLM processes

Reads all log lines
Identifies patterns, errors, anomalies
Generates human-readable analysis

LLM returns analysis

Example: "Transaction failed due to cash dispenser timeout at 12:01:45. Cassette 3 didn't respond. Possible mechanical jam."

Store and return

Saves analysis with metadata (timestamp, model, transaction ID)
Returns to frontend

User provides feedback

Rates analysis 1-5
Selects alternative cause if LLM wrong
Adds comments
Enters passcode for authentication

Feedback stored

Appends to llm_feedback.json file
One line per feedback entry
Also stores in session for immediate viewing

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 7: COUNTER ANALYSIS

User selects source file + transaction

Must have matching TRC trace file

Backend finds TRC trace file

Looks for .prn file with matching date

Extract ALL counter blocks from TRC

TRC files contain multiple "CCdmCashUnitInfoDataEx" sections
Each section shows cassette states at a point in time
Parses each section: cassette number, counts, status

Counter block format:

No: Cassette number (01, 02, 03, etc.)
Val: Current note count
Init: Initial count when loaded
Cnt: Total dispensed
RCnt: Rejected notes
St: Status (OK, Low, Empty)

Map counters to transaction

Gets transaction start and end times
Finds first counter block BEFORE transaction
Finds last counter block AFTER transaction (or at end of file)

Build "Counter per Transaction" table

Lists all transactions from selected one onwards
For each transaction:

Shows if counters exist in its timeframe
"View Counters" link if counters found

When user clicks "View Counters"

Shows snapshot of all cassettes
Displays: cassette name, current count, currency, status

Return all data

First counter (before transactions)
Last counter (after transactions)
Transaction table with counter links

Cassette 1: Had 500 EUR notes → now has 480 (20 dispensed)
Cassette 3: Status changed from OK → Low
Helps diagnose dispenser issues

-----------------------------------------------------------------------------------------------------------------------------------

FEATURE 8: REGISTRY COMPARISON

Single Registry View:

User selects one .reg file
Backend parses file:

Reads with multiple encoding attempts
Finds section headers: [HKEY_...]
Extracts key-value pairs: "Setting"="Value"

Returns as table: Path | Key | Value
Frontend displays with search filter

Registry Comparison:

User uploads SECOND ZIP package
Backend processes it (same extraction/categorization)
User selects file from each package
Backend compares line-by-line:

Finds lines that changed content
Finds lines that only changed whitespace
Finds identical lines

Frontend shows side-by-side with color coding:

Red = content changed
Purple = whitespace only
No color = identical

-----------------------------------------------------------------------------------------------------------------------------------

DATA STORAGE LOCATIONS


Uploaded ZIP:

Temporarily in browser memory
Sent to backend via HTTP
NOT saved permanently anywhere


Extracted files:

/tmp/dn_extracts/dn_1735824000_abc123xyz/
Original structure preserved in ATM_Logs/ subfolder
Organized copies in category subfolders


Session data (in RAM):

   {
     'file_categories': {...},       // Which files exist
     'extraction_path': '...',       // Where files are
     'transaction_data': [...],      // Parsed transactions
     'source_files': [...],          // Source file names
     'feedback_data': [...]          // LLM feedback
   }

LLM feedback (on disk):

llm_feedback.json in current directory
One JSON object per line
Permanent storage


Streamlit session (in browser):

   {
     'zip_processed': True,
     'processing_result': {...},     // File counts
     'last_processed_file': '...',   // Prevent reprocessing
     'analysis_result': {...}        // LLM analysis
   }




FILE LIFECYCLE

Upload → Browser memory
Send to backend → HTTP request body
Extract → /tmp/dn_extracts/.../
Categorize → Identify file types
Organize → Copy to category folders
Parse → Read and analyze content
Store → Save results in session (RAM)
Display → Send to frontend
Cleanup → Auto-delete after 24 hours